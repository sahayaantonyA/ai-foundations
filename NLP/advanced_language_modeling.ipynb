{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e187158",
   "metadata": {},
   "source": [
    "# Advanced Language Modeling Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fa3d25",
   "metadata": {},
   "source": [
    "## 1. Sampling Sentences from Language Models\n",
    "Language models can generate sentences by sampling words based on learned probabilities.\n",
    "\n",
    "**Example:** Predicting the next word in the sentence: \"The cat sat on the...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b146b8",
   "metadata": {},
   "source": [
    "## 2. N-gram Performance\n",
    "Increasing the value of `N` (in N-gram models) generally improves performance.\n",
    "\n",
    "**Example:**\n",
    "- **Unigram:** \"I like cake.\"\n",
    "- **Bigram:** \"I like\" / \"like cake.\"\n",
    "- **Trigram:** \"I like cake.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5af9c0f",
   "metadata": {},
   "source": [
    "## 3. Context and Sentence Coherence\n",
    "Longer contexts in language models produce more coherent and meaningful sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0a597c",
   "metadata": {},
   "source": [
    "## 4. Domain-Specific Training\n",
    "Without specific training, language models may produce nonsensical outputs.\n",
    "\n",
    "**Example:** \"Dogs are juicy and sweet.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa20c62",
   "metadata": {},
   "source": [
    "## 5. Zero Probability Problem\n",
    "Unseen sequences result in zero probabilities, making perplexity computation impossible.\n",
    "\n",
    "**Example:** If \"rare phrase\" never appeared in training, its probability becomes zero, which breaks the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f5b38d",
   "metadata": {},
   "source": [
    "## 6. Smoothing or Discounting Techniques\n",
    "To avoid zero probabilities, smoothing redistributes some probability mass from common events to unseen ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38dac49",
   "metadata": {},
   "source": [
    "## 7. Laplace Smoothing (Add-1 Smoothing)\n",
    "**Formula:** `P(W) = (Count(W) + 1) / (Total Words + Vocabulary Size)`\n",
    "\n",
    "**Example:** With `N = 10,000` words and a vocabulary size `V = 5,000`, if \"rareword\" appears once:\n",
    "\n",
    "`P(rareword) = (1 + 1) / (10,000 + 5,000) = 2 / 15,000`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23291bde",
   "metadata": {},
   "source": [
    "## 8. Discounting\n",
    "The ratio between smoothed and unsmoothed probabilities.\n",
    "Used to control the distribution shift in the smoothing process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8b94f8",
   "metadata": {},
   "source": [
    "## 9. Add-k Smoothing\n",
    "A variant of Laplace smoothing that uses a constant `k` instead of `1`.\n",
    "\n",
    "**Formula:** `P(W) = (Count(W) + k) / (Total Words + k * Vocabulary Size)`\n",
    "\n",
    "**Example:** Choosing `k = 0.5` for less aggressive smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5446f8f6",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Mastering these concepts helps in improving model reliability and performance in NLP tasks. Techniques like smoothing prevent zero probability pitfalls, ensuring robust sentence generation and prediction. ðŸš€"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
