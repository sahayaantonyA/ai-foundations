{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd7e6034",
   "metadata": {},
   "source": [
    "## Introduction to Tokenization in NLP\n",
    "Tokenization is a fundamental step in Natural Language Processing (NLP). It involves breaking down text into smaller units, such as words, subwords, or sentences, making it easier for machines to process and understand language.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcc34de",
   "metadata": {},
   "source": [
    "### **1. Tokenization: Segmentation of Corpus**\n",
    "A corpus is a large collection of text data used for NLP tasks. Tokenization divides this corpus into meaningful units (tokens), such as words or subwords.\n",
    "\n",
    "**Example:**\n",
    "Sentence: \"AI is transforming the world.\"\n",
    "Tokens: `[\"AI\", \"is\", \"transforming\", \"the\", \"world\", \".\"]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae52122",
   "metadata": {},
   "source": [
    "### **2. Rogerian Psychotherapy and Tokenization**\n",
    "Rogerian Psychotherapy is a conversational therapy model where responses are based on user inputs. Early NLP systems like ELIZA used simple tokenization methods to recognize patterns in user conversations.\n",
    "\n",
    "**Example:**\n",
    "User: \"I feel sad.\"\n",
    "ELIZA: \"Why do you feel sad?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3ae1fe",
   "metadata": {},
   "source": [
    "### **3. Subword Tokenization (进入 = 进 + 入)**\n",
    "Languages like Chinese, Japanese, and Korean do not use spaces between words. Instead of word-based tokenization, subword tokenization splits words into smaller units based on meaning.\n",
    "\n",
    "**Example:**\n",
    "Chinese phrase: \"进入\" (meaning \"enter\")\n",
    "Subword Tokens: \"进\" (go) + \"入\" (enter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae6e36d",
   "metadata": {},
   "source": [
    "### **4. Tokenization and Named Entity Recognition (NER)**\n",
    "NER is the process of identifying proper names (e.g., people, places) in text. Tokenization must ensure these names are preserved correctly.\n",
    "\n",
    "**Example:**\n",
    "Sentence: \"Barack Obama was the 44th President of the USA.\"\n",
    "Named Entities: `[\"Barack Obama\", \"USA\"]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66ad2ab",
   "metadata": {},
   "source": [
    "### **5. Penn Treebank Tokenization: Clitics and Punctuation**\n",
    "Penn Treebank is a tokenization standard that handles contractions and punctuation properly.\n",
    "\n",
    "**Example:**\n",
    "Input: \"doesn’t\"\n",
    "Tokens: `[\"does\", \"n’t\"]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4568d5",
   "metadata": {},
   "source": [
    "### **6. Morpheme: Meaningful Segment of a Word**\n",
    "A morpheme is the smallest unit of meaning in a language. Words can have multiple morphemes.\n",
    "\n",
    "**Example:**\n",
    "Word: \"unhappiness\"\n",
    "Morphemes: `[\"un\" (not), \"happy\" (feeling), \"ness\" (state)]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab1d6c9",
   "metadata": {},
   "source": [
    "### **7. Hanzi Forms Morphemes**\n",
    "Hanzi (Chinese characters) form words by combining morphemes. Tokenization of Hanzi must consider meaningful segmentations.\n",
    "\n",
    "**Example:**\n",
    "Sentence: \"姚明进入总决赛\" (Yao Ming enters the finals)\n",
    "Tokens: `[姚 (Yao), 明 (Ming), 进 (enter), 入 (into), 总 (total), 决赛 (finals)]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67506ac8",
   "metadata": {},
   "source": [
    "### **8. Extended/Verbose Mode: (?x)**\n",
    "Regular expressions in NLP can use \"verbose mode\" to make complex tokenization patterns more readable.\n",
    "\n",
    "**Example:**\n",
    "Regex pattern for splitting words and numbers:\n",
    "```\n",
    "(?x)\\b(\\w+)\\b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec1a557",
   "metadata": {},
   "source": [
    "### **9. Tokenization with nltk.regexp_tokenize**\n",
    "NLTK (Natural Language Toolkit) provides a method to tokenize text using regular expressions.\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64527529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "pattern = r'\\w+'\n",
    "text = \"Tokenization is fun!\"\n",
    "nltk.regexp_tokenize(text, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca1d616",
   "metadata": {},
   "source": [
    "### **10. Token Learner and Token Trainer**\n",
    "Token learners and trainers are algorithms used to learn optimal tokenization strategies from large datasets.\n",
    "\n",
    "**Example:**\n",
    "A machine learning model trained to recognize how words are split in different languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20798af1",
   "metadata": {},
   "source": [
    "### **11. Top-down Tokenization**\n",
    "In this method, text is split into large segments first and then refined into smaller tokens.\n",
    "\n",
    "**Example:**\n",
    "1. Split text into sentences\n",
    "2. Split sentences into words\n",
    "3. Split words into subwords (if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d391be0",
   "metadata": {},
   "source": [
    "### **12. Bottom-up Tokenization**\n",
    "Bottom-up tokenization starts with the smallest units (characters) and gradually groups them into words and phrases.\n",
    "\n",
    "**Example:**\n",
    "1. Start with characters `[\"c\", \"a\", \"t\"]`\n",
    "2. Merge into words `[\"cat\"]`\n",
    "3. Merge into phrases `[\"the cat\"]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84220dc",
   "metadata": {},
   "source": [
    "### **Conclusion**\n",
    "Tokenization is a crucial step in NLP, affecting text processing accuracy. Different approaches suit different languages and applications. Understanding these methods helps in building effective AI-powered language models."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
