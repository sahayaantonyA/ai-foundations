{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities of All Unigrams: :  {'the': 0.08771929824561403, 'cat': 0.017543859649122806, 'sat': 0.017543859649122806, 'on': 0.017543859649122806, 'mat.': 0.017543859649122806, 'dogs': 0.017543859649122806, 'are': 0.03508771929824561, 'great': 0.017543859649122806, 'companions.': 0.017543859649122806, 'i': 0.03508771929824561, 'love': 0.017543859649122806, 'to': 0.05263157894736842, 'play': 0.017543859649122806, 'with': 0.017543859649122806, 'my': 0.017543859649122806, 'dog.': 0.017543859649122806, 'cats': 0.017543859649122806, 'independent': 0.017543859649122806, 'animals.': 0.017543859649122806, 'pets': 0.017543859649122806, 'bring': 0.017543859649122806, 'joy': 0.017543859649122806, 'families.': 0.017543859649122806, 'sun': 0.017543859649122806, 'is': 0.03508771929824561, 'shining': 0.017543859649122806, 'brightly': 0.017543859649122806, 'today.': 0.017543859649122806, 'enjoy': 0.017543859649122806, 'reading': 0.017543859649122806, 'books': 0.017543859649122806, 'in': 0.017543859649122806, 'park.': 0.017543859649122806, 'weather': 0.017543859649122806, 'nice': 0.017543859649122806, 'for': 0.017543859649122806, 'a': 0.017543859649122806, 'walk.': 0.017543859649122806, 'she': 0.017543859649122806, 'loves': 0.017543859649122806, 'cook': 0.017543859649122806, 'delicious': 0.017543859649122806, 'meals.': 0.017543859649122806, 'he': 0.017543859649122806, 'plays': 0.017543859649122806, 'soccer': 0.017543859649122806, 'every': 0.017543859649122806, 'weekend.': 0.017543859649122806}\n",
      "Maxium Probability :  the\n",
      "Predicted Next Word:  the\n",
      "Perplexity:  41.20466731156503\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "class UnigramModel:\n",
    "    def __init__(self,corpus_path: str):\n",
    "        self.corpus_path = corpus_path\n",
    "    \n",
    "    def get_tokenized_sentences(self, file_path: str):\n",
    "        with open(file= file_path, mode= 'r', encoding= 'utf-8') as f:\n",
    "            return [sentance.strip().lower().split() for sentance in f]        \n",
    "        \n",
    "    def get_words_count(self,tokenized_sentences: list[list[str]]):\n",
    "        words_count = {}\n",
    "        for sentence in tokenized_sentences:\n",
    "            for i in range(len(sentence)):\n",
    "                w1 = sentence[i]\n",
    "                if w1 not in words_count:\n",
    "                    words_count[w1] = 0\n",
    "                words_count[w1] += 1\n",
    "        return words_count\n",
    "    \n",
    "    def compute_probs(self,words_count: dict):\n",
    "        return {w1: count/ sum(words_count.values()) for w1, count in words_count.items()}\n",
    "    \n",
    "    def generate_next_word(self):\n",
    "        return max(self.probs,key= self.probs.get)\n",
    "    \n",
    "    def build(self):\n",
    "        tokenized_sentences = self.get_tokenized_sentences(file_path= self.corpus_path)\n",
    "        words_count = self.get_words_count(tokenized_sentences= tokenized_sentences)\n",
    "        self.probs = self.compute_probs(words_count= words_count)\n",
    "        print(\"Probabilities of All Unigrams: : \",self.probs)\n",
    "        print(\"Maxium Probability : \", max(self.probs,key= self.probs.get))\n",
    "\n",
    "    def compute_perplexity(self,test_corpus_path: str):\n",
    "        total_word_counts = 0\n",
    "        total_log_prob = 0\n",
    "        sentences = self.get_tokenized_sentences(file_path= test_corpus_path)\n",
    "        for sentence in sentences:\n",
    "            sentence_prob = 0\n",
    "            for word in sentence:\n",
    "                prob = self.probs.get(word, 1 / (sum(self.probs.values()) + len(self.probs)))\n",
    "                sentence_prob += math.log(prob)\n",
    "            total_word_counts += len(sentence) \n",
    "            total_log_prob += sentence_prob\n",
    "        return math.exp(-total_log_prob/total_word_counts) # PP(w) = exp(-1/N * ∑log P(w_i|w_previous_words))\n",
    "        \n",
    "        \n",
    "model = UnigramModel(corpus_path= 'ex_corpus.txt')\n",
    "model.build()\n",
    "print(\"Predicted Next Word: \", model.generate_next_word())\n",
    "print(\"Perplexity: \", model.compute_perplexity(test_corpus_path= 'test_corpus.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Probs:  {'the': {'cat': 0.2, 'mat.': 0.2, 'sun': 0.2, 'park.': 0.2, 'weather': 0.2}, 'cat': {'sat': 1.0}, 'sat': {'on': 1.0}, 'on': {'the': 1.0}, 'dogs': {'are': 1.0}, 'are': {'great': 0.5, 'independent': 0.5}, 'great': {'companions.': 1.0}, 'i': {'love': 0.5, 'enjoy': 0.5}, 'love': {'to': 1.0}, 'to': {'play': 0.3333333333333333, 'families.': 0.3333333333333333, 'cook': 0.3333333333333333}, 'play': {'with': 1.0}, 'with': {'my': 1.0}, 'my': {'dog.': 1.0}, 'cats': {'are': 1.0}, 'independent': {'animals.': 1.0}, 'pets': {'bring': 1.0}, 'bring': {'joy': 1.0}, 'joy': {'to': 1.0}, 'sun': {'is': 1.0}, 'is': {'shining': 0.5, 'nice': 0.5}, 'shining': {'brightly': 1.0}, 'brightly': {'today.': 1.0}, 'enjoy': {'reading': 1.0}, 'reading': {'books': 1.0}, 'books': {'in': 1.0}, 'in': {'the': 1.0}, 'weather': {'is': 1.0}, 'nice': {'for': 1.0}, 'for': {'a': 1.0}, 'a': {'walk.': 1.0}, 'she': {'loves': 1.0}, 'loves': {'to': 1.0}, 'cook': {'delicious': 1.0}, 'delicious': {'meals.': 1.0}, 'he': {'plays': 1.0}, 'plays': {'soccer': 1.0}, 'soccer': {'every': 1.0}, 'every': {'weekend.': 1.0}}\n",
      "cat sat on the cat sat on the cat sat on \n",
      "cat sat on the cat sat on the cat sat on \n",
      "Perplexity:  25.332882490399903\n"
     ]
    }
   ],
   "source": [
    "class BigramModel:\n",
    "    def __init__(self, corpus_path: str):\n",
    "        self.corpus_path = corpus_path\n",
    "\n",
    "    def tokenization(self, file_path: str):\n",
    "        with open(file= file_path, mode= 'r', encoding= 'utf-8') as file:\n",
    "            return [line.lower().strip().split() for line in file if line.strip()]    \n",
    "    \n",
    "    def get_words_count(self,sentences_tokens: list[list[str]]):\n",
    "        from collections import Counter\n",
    "        self.unique_word_count = Counter(word for sentence in sentences_tokens for word in sentence)\n",
    "        bigram_count = {}\n",
    "        for sentence_tokens in sentences_tokens:\n",
    "            for i in range(len(sentence_tokens) - 1):\n",
    "                w1,w2 = sentence_tokens[i], sentence_tokens[i+1]\n",
    "                if w1 not in bigram_count:\n",
    "                    bigram_count[w1] = {}\n",
    "                if w2 not in bigram_count[w1]:\n",
    "                    bigram_count[w1][w2] = 0\n",
    "                bigram_count[w1][w2] += 1\n",
    "        return bigram_count\n",
    "    \n",
    "    def compute_probs(self,bigram_counts: dict):\n",
    "        return {w1 : {w2 : count/self.unique_word_count[w1] for w2, count in w2_count.items()} for w1, w2_count in bigram_counts.items()}\n",
    "\n",
    "    def generate_next_words(self,input_query: str):\n",
    "        import time\n",
    "        input_query = input_query.lower().strip().split()\n",
    "        last_word = input_query[-1]\n",
    "        if last_word in self.bigram_probs:\n",
    "            for _ in range(10):\n",
    "                if last_word in self.bigram_probs:\n",
    "                    next_word = max(self.bigram_probs[input_query[-1]], key= self.bigram_probs[input_query[-1]].get)\n",
    "                    input_query.append(next_word)\n",
    "                    last_word = next_word\n",
    "            for word in input_query:\n",
    "                print(word, end=' ', flush= True)\n",
    "                time.sleep(0.5)\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"Sorry, I couldn't find predictions for '{last_word}'. Try another word!\") \n",
    "\n",
    "    def compute_perplexity(self,test_corpus_path: str):\n",
    "        total_word_counts = 0\n",
    "        total_log_prob = 0\n",
    "        sentences = self.tokenization(file_path= test_corpus_path)\n",
    "        for sentence in sentences:\n",
    "            sentence_prob = 0\n",
    "            for i in range(len(sentence)-1):\n",
    "                prob = (self.bigram_probs.get(sentence[i], {}).get(sentence[i+1], 0) + 1) / (sum(self.bigram_probs.get(sentence[i], {}).values()) + len(self.unique_word_count))\n",
    "                sentence_prob += math.log(prob)\n",
    "            total_word_counts += len(sentence) \n",
    "            total_log_prob += sentence_prob\n",
    "        return math.exp(-total_log_prob/total_word_counts) # PP(w) = exp(-1/N * ∑log P(w_i|w_previous_words))\n",
    "    \n",
    "    def build(self):\n",
    "        sentences = self.tokenization(file_path= self.corpus_path)\n",
    "        bigram_counts = self.get_words_count(sentences_tokens= sentences)\n",
    "        self.bigram_probs = self.compute_probs(bigram_counts= bigram_counts)\n",
    "        print(\"Bigram Probs: \", self.bigram_probs)\n",
    "\n",
    "model = BigramModel(corpus_path= 'ex_corpus.txt')\n",
    "model.build()\n",
    "\n",
    "while True:\n",
    "    query = input('You: ')\n",
    "    if query.lower() != 'exit':\n",
    "        model.generate_next_words(input_query= query)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(\"Perplexity: \", model.compute_perplexity(test_corpus_path= 'test_corpus.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat sat on the mat. \n"
     ]
    }
   ],
   "source": [
    "class TrigramModel:\n",
    "    def __init__(self, corpus_path: str):\n",
    "        self.corpus_path = corpus_path\n",
    "\n",
    "    def tokenization(self):\n",
    "        with open(file= self.corpus_path, mode= 'r', encoding= 'utf-8') as file:\n",
    "            return [line.lower().strip().split() for line in file if line.strip()]    \n",
    "        \n",
    "    def get_words_count(self, sentences_tokens: list[list[str]]):\n",
    "        trigram_count = {}\n",
    "        for sentence_tokens in sentences_tokens:\n",
    "            for i in range(len(sentence_tokens) - 2):\n",
    "                w1, w2, w3 = sentence_tokens[i], sentence_tokens[i+1], sentence_tokens[i+2]\n",
    "                if w1 not in trigram_count:\n",
    "                    trigram_count[w1] = {}\n",
    "                if w2 not in trigram_count[w1]:\n",
    "                    trigram_count[w1][w2] = {}\n",
    "                if w3 not in trigram_count[w1][w2]:\n",
    "                    trigram_count[w1][w2][w3] = 0\n",
    "                trigram_count[w1][w2][w3] += 1\n",
    "        return trigram_count\n",
    "    \n",
    "    def compute_probs(self, trigram_counts: dict):\n",
    "        return {w1: {w2: {w3: count / sum(trigram_counts[w1][w2].values())  for w3, count in trigram_counts[w1][w2].items()} for w2 in trigram_counts[w1]}for w1 in trigram_counts}\n",
    "\n",
    "    def generate_next_word(self, input_query: str):\n",
    "        input_query = input_query.lower().strip().split()\n",
    "        if input_query.__len__() >= 2:\n",
    "            last_two_words = input_query[-2:]\n",
    "            if last_two_words[-2] in self.trigram_probs and  last_two_words[-1] in self.trigram_probs[last_two_words[-2]]:\n",
    "                for _ in range(10):\n",
    "                    if last_two_words[-2] in self.trigram_probs and last_two_words[-1] in self.trigram_probs[last_two_words[-2]]:\n",
    "                        next_word = max(self.trigram_probs[last_two_words[-2]][last_two_words[-1]], key=self.trigram_probs[last_two_words[-2]][last_two_words[-1]].get)\n",
    "                        input_query.append(next_word)\n",
    "                        last_two_words = input_query[-2:]\n",
    "                for word in input_query:\n",
    "                    import time\n",
    "                    print(word, end=' ', flush= True)\n",
    "                    time.sleep(0.5)\n",
    "                print()\n",
    "            else:\n",
    "                print(f\"Sorry, I couldn't find predictions for '{last_two_words[0]}' and '{last_two_words[1]}'. Try another word!\") \n",
    "\n",
    "        else:\n",
    "            print(\"Need at least two words!\")\n",
    "\n",
    "    def build(self):\n",
    "        tokens = self.tokenization()\n",
    "        words_count = self.get_words_count(sentences_tokens= tokens)\n",
    "        self.trigram_probs = self.compute_probs(trigram_counts= words_count)\n",
    "\n",
    "model = TrigramModel(corpus_path= 'ex_corpus.txt')\n",
    "model.build()\n",
    "\n",
    "while True:\n",
    "    query = input('You: ')\n",
    "    if query.lower() != 'exit':\n",
    "        model.generate_next_word(input_query= query)\n",
    "    else:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class UnigramModel:\n",
    "    def __init__(self,corpus_path: str):\n",
    "        self.corpus_path = corpus_path\n",
    "    \n",
    "    def get_tokenized_sentences(self, file_path: str):\n",
    "        with open(file= file_path, mode= 'r', encoding= 'utf-8') as f:\n",
    "            return [sentance.strip().lower().split() for sentance in f]        \n",
    "        \n",
    "    def get_words_count(self,tokenized_sentences: list[list[str]]):\n",
    "        words_count = {}\n",
    "        for sentence in tokenized_sentences:\n",
    "            for i in range(len(sentence)):\n",
    "                w1 = sentence[i]\n",
    "                if w1 not in words_count:\n",
    "                    words_count[w1] = 0\n",
    "                words_count[w1] += 1\n",
    "        return words_count\n",
    "    \n",
    "    def compute_probs(self,words_count: dict):\n",
    "        return {w1: count/ sum(words_count.values()) for w1, count in words_count.items()}\n",
    "    \n",
    "    def generate_next_word(self):\n",
    "        return max(self.probs,key= self.probs.get)\n",
    "    \n",
    "    def build(self):\n",
    "        tokenized_sentences = self.get_tokenized_sentences(file_path= self.corpus_path)\n",
    "        words_count = self.get_words_count(tokenized_sentences= tokenized_sentences)\n",
    "        self.probs = self.compute_probs(words_count= words_count)\n",
    "        print(\"Probabilities of All Unigrams: : \",self.probs)\n",
    "        print(\"Maxium Probability : \", max(self.probs,key= self.probs.get))\n",
    "\n",
    "    def compute_perplexity(self,test_corpus_path: str):\n",
    "        sentence_prob = 0\n",
    "        total_word_counts = 0\n",
    "        total_log_prob = 0\n",
    "        sentences = self.get_tokenized_sentences(file_path= test_corpus_path)\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                prob = self.probs.get(word,1e-6)\n",
    "                sentence_prob += math.log(prob)\n",
    "            total_word_counts += len(sentence) \n",
    "            total_log_prob += sentence_prob\n",
    "        return math.exp(-total_log_prob/total_word_counts) # PP(w) = exp(-1/N * ∑log P(w_i|w_previous_words))\n",
    "        \n",
    "        \n",
    "model = UnigramModel(corpus_path= 'ex_corpus.txt')\n",
    "model.build()\n",
    "print(\"Predicted Next Word: \", model.generate_next_word())\n",
    "print(\"Perplexity: \", model.compute_perplexity(test_corpus_path= 'test_corpus.txt'))\n",
    "\n",
    "\n",
    "class BigramModel:\n",
    "    def __init__(self, corpus_path: str):\n",
    "        self.corpus_path = corpus_path\n",
    "\n",
    "    def tokenization(self, file_path: str):\n",
    "        with open(file= file_path, mode= 'r', encoding= 'utf-8') as file:\n",
    "            return [line.lower().strip().split() for line in file if line.strip()]    \n",
    "    \n",
    "    def get_words_count(self,sentences_tokens: list[list[str]]):\n",
    "        from collections import Counter\n",
    "        unique_word_count = Counter(word for sentence in sentences_tokens for word in sentence)\n",
    "        bigram_count = {}\n",
    "        for sentence_tokens in sentences_tokens:\n",
    "            for i in range(len(sentence_tokens) - 1):\n",
    "                w1,w2 = sentence_tokens[i], sentence_tokens[i+1]\n",
    "                if w1 not in bigram_count:\n",
    "                    bigram_count[w1] = {}\n",
    "                if w2 not in bigram_count[w1]:\n",
    "                    bigram_count[w1][w2] = 0\n",
    "                bigram_count[w1][w2] += 1\n",
    "        return unique_word_count, bigram_count\n",
    "    \n",
    "    def compute_probs(self, unique_word_count: dict, bigram_counts: dict):\n",
    "        return {w1 : {w2 : count/unique_word_count[w1] for w2, count in w2_count.items()} for w1, w2_count in bigram_counts.items()}\n",
    "\n",
    "    def generate_next_words(self,input_query: str):\n",
    "        import time\n",
    "        input_query = input_query.lower().strip().split()\n",
    "        last_word = input_query[-1]\n",
    "        if last_word in self.bigram_probs:\n",
    "            for _ in range(10):\n",
    "                if last_word in self.bigram_probs:\n",
    "                    next_word = max(self.bigram_probs[input_query[-1]], key= self.bigram_probs[input_query[-1]].get)\n",
    "                    input_query.append(next_word)\n",
    "                    last_word = next_word\n",
    "            for word in input_query:\n",
    "                print(word, end=' ', flush= True)\n",
    "                time.sleep(0.5)\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"Sorry, I couldn't find predictions for '{last_word}'. Try another word!\") \n",
    "\n",
    "    def compute_perplexity(self,test_corpus_path: str):\n",
    "        sentence_prob = 0\n",
    "        total_word_counts = 0\n",
    "        total_log_prob = 0\n",
    "        sentences = self.tokenization(file_path= test_corpus_path)\n",
    "        for sentence in sentences:\n",
    "            for i in range(len(sentence)-1):\n",
    "                prob = self.bigram_probs.get(sentence[i],{}).get(sentence[i+1],1e-6)\n",
    "                sentence_prob += math.log(prob)\n",
    "            total_word_counts += len(sentence) \n",
    "            total_log_prob += sentence_prob\n",
    "        return math.exp(-total_log_prob/total_word_counts) # PP(w) = exp(-1/N * ∑log P(w_i|w_previous_words))\n",
    "    \n",
    "    def build(self):\n",
    "        sentences = self.tokenization(file_path= self.corpus_path)\n",
    "        unique_word_count, bigram_counts = self.get_words_count(sentences_tokens= sentences)\n",
    "        self.bigram_probs = self.compute_probs(unique_word_count= unique_word_count, bigram_counts= bigram_counts)\n",
    "        print(\"Bigram Probs: \", self.bigram_probs)\n",
    "\n",
    "model = BigramModel(corpus_path= 'ex_corpus.txt')\n",
    "model.build()\n",
    "\n",
    "while True:\n",
    "    query = input('You: ')\n",
    "    if query.lower() != 'exit':\n",
    "        model.generate_next_words(input_query= query)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(\"Perplexity: \", model.compute_perplexity(test_corpus_path= 'test_corpus.txt'))\n",
    "\n",
    "\n",
    "\n",
    "class TrigramModel:\n",
    "    def __init__(self, corpus_path: str):\n",
    "        self.corpus_path = corpus_path\n",
    "\n",
    "    def tokenization(self,file_path: str):\n",
    "        with open(file= file_path, mode= 'r', encoding= 'utf-8') as file:\n",
    "            return [line.lower().strip().split() for line in file if line.strip()]    \n",
    "        \n",
    "    def get_words_count(self, sentences_tokens: list[list[str]]):\n",
    "        trigram_count = {}\n",
    "        for sentence_tokens in sentences_tokens:\n",
    "            for i in range(len(sentence_tokens) - 2):\n",
    "                w1, w2, w3 = sentence_tokens[i], sentence_tokens[i+1], sentence_tokens[i+2]\n",
    "                if w1 not in trigram_count:\n",
    "                    trigram_count[w1] = {}\n",
    "                if w2 not in trigram_count[w1]:\n",
    "                    trigram_count[w1][w2] = {}\n",
    "                if w3 not in trigram_count[w1][w2]:\n",
    "                    trigram_count[w1][w2][w3] = 0\n",
    "                trigram_count[w1][w2][w3] += 1\n",
    "        return trigram_count\n",
    "    \n",
    "    def compute_probs(self, trigram_counts: dict):\n",
    "        return {w1: {w2: {w3: count / sum(trigram_counts[w1][w2].values())  for w3, count in trigram_counts[w1][w2].items()} for w2 in trigram_counts[w1]}for w1 in trigram_counts}\n",
    "\n",
    "    def generate_next_word(self, input_query: str):\n",
    "        input_query = input_query.lower().strip().split()\n",
    "        if input_query.__len__() >= 2:\n",
    "            last_two_words = input_query[-2:]\n",
    "            if last_two_words[-2] in self.trigram_probs and  last_two_words[-1] in self.trigram_probs[last_two_words[-2]]:\n",
    "                for _ in range(10):\n",
    "                    if last_two_words[-2] in self.trigram_probs and last_two_words[-1] in self.trigram_probs[last_two_words[-2]]:\n",
    "                        next_word = max(self.trigram_probs[last_two_words[-2]][last_two_words[-1]], key=self.trigram_probs[last_two_words[-2]][last_two_words[-1]].get)\n",
    "                        input_query.append(next_word)\n",
    "                        last_two_words = input_query[-2:]\n",
    "                for word in input_query:\n",
    "                    import time\n",
    "                    print(word, end=' ', flush= True)\n",
    "                    time.sleep(0.5)\n",
    "                print()\n",
    "            else:\n",
    "                print(f\"Sorry, I couldn't find predictions for '{last_two_words[0]}' and '{last_two_words[1]}'. Try another word!\") \n",
    "\n",
    "        else:\n",
    "            print(\"Need at least two words!\")\n",
    "\n",
    "    def compute_perplexity(self,test_corpus_path: str):\n",
    "        sentence_prob = 0\n",
    "        total_word_counts = 0\n",
    "        total_log_prob = 0\n",
    "        sentences = self.tokenization(file_path= test_corpus_path)\n",
    "        for sentence in sentences:\n",
    "            for i in range(len(sentence)-2):\n",
    "                prob = self.trigram_probs.get(sentence[i],{}).get(sentence[i+1],{}).get(sentence[i+2],1e-6)\n",
    "                sentence_prob += math.log(prob)\n",
    "            total_word_counts += len(sentence) \n",
    "            total_log_prob += sentence_prob\n",
    "        return math.exp(-total_log_prob/total_word_counts) # PP(w) = exp(-1/N * ∑log P(w_i|w_previous_words))\n",
    "    \n",
    "    def build(self):\n",
    "        tokens = self.tokenization(file_path= self.corpus_path)\n",
    "        words_count = self.get_words_count(sentences_tokens= tokens)\n",
    "        self.trigram_probs = self.compute_probs(trigram_counts= words_count)\n",
    "\n",
    "model = TrigramModel(corpus_path= 'ex_corpus.txt')\n",
    "model.build()\n",
    "\n",
    "while True:\n",
    "    query = input('You: ')\n",
    "    if query.lower() != 'exit':\n",
    "        model.generate_next_word(input_query= query)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(\"Perplexity: \", model.compute_perplexity(test_corpus_path= 'test_corpus.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {('<s>', 'i'): 2, ('i', 'love'): 2, ('love', 'nlp'): 1, ('nlp', '</s>'): 1, ('<s>', 'nlp'): 1, ('nlp', 'is'): 1, ('is', 'amazing'): 1, ('amazing', '</s>'): 1, ('love', 'python'): 1, ('python', '</s>'): 1})\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocess_text(text:str):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]','',text)\n",
    "    tokens = word_tokenize(text)\n",
    "    print(tokens)\n",
    "\n",
    "# text = \"I love Natural Language Processing! It's amazing.\"\n",
    "# tokens = preprocess_text(text)\n",
    "# print(tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {('<s>',): 3,\n",
       "             ('i',): 2,\n",
       "             ('love',): 2,\n",
       "             ('nlp',): 2,\n",
       "             ('is',): 1,\n",
       "             ('amazing',): 1,\n",
       "             ('python',): 1})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model.get_ngram_prob(ngram=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {('<s>', 'i'): 2,\n",
       "             ('i', 'love'): 2,\n",
       "             ('love', 'nlp'): 1,\n",
       "             ('nlp', '</s>'): 1,\n",
       "             ('<s>', 'nlp'): 1,\n",
       "             ('nlp', 'is'): 1,\n",
       "             ('is', 'amazing'): 1,\n",
       "             ('amazing', '</s>'): 1,\n",
       "             ('love', 'python'): 1,\n",
       "             ('python', '</s>'): 1})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model.ngram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5-2+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : 3 Gram\n",
      "Perplxity Score :  24.38337832186403\n",
      "i saw one cat \n",
      "------------------\n",
      "Model : 3 Gram\n",
      "Perplxity Score :  24.38337832186403\n",
      "i saw one cat sat on the mat \n",
      "------------------\n",
      "Model : 3 Gram\n",
      "Perplxity Score :  24.38337832186403\n",
      "i know she loves to cook delicious meals \n",
      "------------------\n",
      "Model : 6 Gram\n",
      "Perplxity Score :  12.412640606314818\n",
      "i know she loves \n",
      "------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'exit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[134], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(sentence[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]) \n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 69\u001b[0m     ngram_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter the gram or 0 to exit: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     input_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter the Input Word : \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ngram_input \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'exit'"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re, math, random, time\n",
    "\n",
    "class NGramModel:\n",
    "    def __init__(self,gram: int, corpus_path: str):\n",
    "        self.n = gram\n",
    "        self.corpus_path = corpus_path\n",
    "        self.ngram_count = defaultdict(int)\n",
    "        self.context_count = defaultdict(int)\n",
    "        self.vocob = set()\n",
    "    \n",
    "    def tokenization(self, corpus_path: str):\n",
    "        tokens = []\n",
    "        with open(file= corpus_path, mode= 'r', encoding= 'utf-8') as f:\n",
    "            for line in f:\n",
    "                processed_text = re.sub(r'[^a-z\\s]','',line.lower())\n",
    "                tokens.append(word_tokenize(processed_text))\n",
    "        return tokens\n",
    "    \n",
    "    def train(self):\n",
    "        sentences_tokens = self.tokenization(corpus_path= self.corpus_path)\n",
    "        for sentence_tokens in sentences_tokens:\n",
    "            tokens = ['<s>'] * (self.n-1) + sentence_tokens + ['<\\s>']\n",
    "            for i in range(len(tokens) - self.n + 1):\n",
    "                ngram = tuple(tokens[i:i+self.n])\n",
    "                context = tuple(tokens[i:i+self.n-1])\n",
    "                self.ngram_count[ngram] += 1\n",
    "                self.context_count[context] += 1\n",
    "            self.vocob.update(tokens)\n",
    "\n",
    "    def get_prob(self,word):\n",
    "        context = word[:-1]\n",
    "        return (self.ngram_count[word] + 1) / (self.context_count[context] + len(self.vocob))\n",
    "\n",
    "    def compute_perplexity(self,test_corpus_path: str):\n",
    "        total_log_prob = 0\n",
    "        total_word_count = 0\n",
    "        sentences_token = self.tokenization(corpus_path= test_corpus_path)\n",
    "        for sentence_token in sentences_token:\n",
    "            token = ['<s>'] * (self.n + 1) + sentence_token + ['</s>']\n",
    "            sentence_prob = 0\n",
    "            for i in range(len(token) - self.n + 1):\n",
    "                ngram = tuple(token[i:i+self.n])\n",
    "                prob = self.get_prob(word=ngram)\n",
    "                sentence_prob += math.log(prob)\n",
    "            total_log_prob += sentence_prob\n",
    "            total_word_count += len(token)\n",
    "        return math.exp(-total_log_prob / total_word_count)\n",
    "    \n",
    "    def generate_text(self, input_words: str,max_length=10):\n",
    "        sentence = [\"<s>\"] * (self.n - 1) + input_words.split()\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            context = tuple(sentence[-(self.n-1):])  # Get the last (n-1) words\n",
    "            possible_ngrams = {ngram: prob for ngram, prob in self.ngram_count.items() if ngram[:-1] == context}\n",
    "            \n",
    "            if not possible_ngrams:\n",
    "                break  # No valid n-grams left\n",
    "            \n",
    "            next_word = random.choices(list(possible_ngrams.keys()), weights=list(possible_ngrams.values()))[0][-1]\n",
    "            if next_word == \"</s>\":\n",
    "                break\n",
    "            \n",
    "            sentence.append(next_word)\n",
    "        return \" \".join(sentence[self.n-1:]) \n",
    "\n",
    "while True:\n",
    "    ngram_input = int(input(\"Enter the gram or 0 to exit: \"))\n",
    "    input_word = input(\"Enter the Input Word : \")\n",
    "    if ngram_input != 0:\n",
    "        ngram = NGramModel(gram= ngram_input, corpus_path= 'ex_corpus.txt')\n",
    "        ngram.train()\n",
    "        print(f\"Model : {ngram_input} Gram\")\n",
    "        perplxity_score = ngram.compute_perplexity(test_corpus_path= 'test_corpus.txt')\n",
    "        print(\"Perplxity Score : \", perplxity_score)\n",
    "        genertaed_sentence = ngram.generate_text(input_words= input_word).replace('<\\s>','')\n",
    "        # print(\"Generated Sentence : \", genertaed_sentence)\n",
    "        for word in genertaed_sentence.split():  # Split the sentence into words\n",
    "            print(word, end=' ', flush=True)  # Print the word followed by a space\n",
    "            time.sleep(0.5)\n",
    "        print()\n",
    "        print('------------------')\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : 2 Gram\n",
      "Perplxity Score :  32.91145145356951\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "generate_text() missing 1 required positional argument: 'input_words'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[127], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m perplxity_score \u001b[38;5;241m=\u001b[39m ngram\u001b[38;5;241m.\u001b[39mcompute_perplexity(test_corpus_path\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_corpus.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerplxity Score : \u001b[39m\u001b[38;5;124m\"\u001b[39m, perplxity_score)\n\u001b[1;32m----> 9\u001b[0m genertaed_sentence \u001b[38;5;241m=\u001b[39m \u001b[43mngram\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms>\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Sentence : \u001b[39m\u001b[38;5;124m\"\u001b[39m, genertaed_sentence)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: generate_text() missing 1 required positional argument: 'input_words'"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    ngram_input = int(input(\"Enter the gram or 0 to exit: \"))\n",
    "    if ngram_input != 0:\n",
    "        ngram = NGramModel(gram= ngram_input, corpus_path= 'ex_corpus.txt')\n",
    "        ngram.train()\n",
    "        print(f\"Model : {ngram_input} Gram\")\n",
    "        perplxity_score = ngram.compute_perplexity(test_corpus_path= 'test_corpus.txt')\n",
    "        print(\"Perplxity Score : \", perplxity_score)\n",
    "        genertaed_sentence = ngram.generate_text(input_words= input_word).replace('<\\s>','')\n",
    "        print(\"Generated Sentence : \", genertaed_sentence)\n",
    "        print('------------------')\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Win a free lottery now!    spam']\n"
     ]
    }
   ],
   "source": [
    "with open('ex_corpus.txt','r') as file:\n",
    "    for line in file:\n",
    "        print(line.strip().rsplit(r'\\t', 1))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
