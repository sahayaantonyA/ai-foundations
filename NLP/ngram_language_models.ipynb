{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "756859ce",
   "metadata": {},
   "source": [
    "# Understanding n-Gram Language Models\n",
    "\n",
    "## What is an n-Gram Language Model?\n",
    "An **n-gram language model** is a way to predict the next word in a sentence based on the previous words. It helps in speech recognition, autocomplete features, and many AI-based text generation systems.\n",
    "\n",
    "For example, if we have the sentence:\n",
    "\n",
    "**\"I love\"**, an n-gram model can predict the next word based on probability, such as:\n",
    "- \"you\" (most likely)\n",
    "- \"chocolate\"\n",
    "- \"math\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d29789",
   "metadata": {},
   "source": [
    "## Probability in Language Models\n",
    "Mathematically, we express this as:\n",
    "\n",
    "**P(w | h)**\n",
    "\n",
    "Where:\n",
    "- **w** is the next word we want to predict.\n",
    "- **h** is the history (previous words).\n",
    "- **P(w | h)** is the probability of word **w** appearing after history **h**.\n",
    "\n",
    "### Example:\n",
    "If we have the history **\"I love\"**, and we are trying to predict the next word **\"math\"**, the probability is:\n",
    "\n",
    "P(\"math\" | \"I love\") = Number of times \"I love\" is followed by \"math\" ÷ Total times \"I love\" appears."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f948a6",
   "metadata": {},
   "source": [
    "## Why is Language Modeling Important?\n",
    "Language is **creative**, meaning there are endless ways to form sentences. Computers need a way to **predict** the next word efficiently, and n-grams provide a simple yet powerful approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b016fc50",
   "metadata": {},
   "source": [
    "## Different Types of n-Gram Models\n",
    "\n",
    "### 1. Unigram Model (n = 1)\n",
    "- The probability of each word is independent of previous words.\n",
    "- Example:\n",
    "  P(\"I love math\") = P(\"I\") × P(\"love\") × P(\"math\")\n",
    "\n",
    "### 2. Bigram Model (n = 2)\n",
    "- The probability of a word depends only on the previous word.\n",
    "- Example:\n",
    "  P(\"I love math\") = P(\"I\") × P(\"love\" | \"I\") × P(\"math\" | \"love\")\n",
    "\n",
    "### 3. Trigram Model (n = 3)\n",
    "- The probability of a word depends on the two previous words.\n",
    "- Example:\n",
    "  P(\"I love math\") = P(\"I\") × P(\"love\" | \"I\") × P(\"math\" | \"I love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ea64ad",
   "metadata": {},
   "source": [
    "## Markov Assumption\n",
    "The **Markov assumption** states that a word's probability depends only on a fixed number of previous words.\n",
    "\n",
    "- **Bigram model** assumes a word depends only on the **1 previous word** (1st-order Markov model).\n",
    "- **Trigram model** assumes a word depends only on the **2 previous words** (2nd-order Markov model).\n",
    "\n",
    "### Example:\n",
    "P(\"Jingle, Bells, Jingle\") = 58% (Markov Assumption: depends on limited history)\n",
    "P(\"Jingle, Bells, Jingle, Bells, Jingle\") = 60% (Using full n-gram model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a100a54e",
   "metadata": {},
   "source": [
    "## Joint Probability of a Sentence\n",
    "The probability of an entire sentence is calculated using:\n",
    "\n",
    "P(w1, w2, ..., wn) = P(w1) × P(w2 | w1) × P(w3 | w1, w2) × ... × P(wn | w1:n-1)\n",
    "\n",
    "### Example:\n",
    "P(\"I love math\") = P(\"I\") × P(\"love\" | \"I\") × P(\"math\" | \"I love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38567272",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- **n-gram models** predict the next word using probability.\n",
    "- **Unigram** models assume independence of words.\n",
    "- **Bigram** models use the previous word for prediction.\n",
    "- **Trigram** models use the previous two words for prediction.\n",
    "- The **Markov assumption** simplifies computations by considering limited history.\n",
    "\n",
    "These models form the basis of many AI applications such as **chatbots, speech recognition, and autocomplete systems**."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
